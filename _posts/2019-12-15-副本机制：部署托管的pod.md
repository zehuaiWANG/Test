---
layout:     post
title:      "副本机制和其他副本机制：部署托管的pod"
subtitle:   " \"Kubernetes\""
date:       2019-12-15 12:00:00
author:     "edsel"
header-img: "img/2.png"
catalog: true
tags:
    - kubernetes
---

> “Pod是kubernetes中的基本部署单元，我们可以直接创建、监督和管理一个pod。kubernetes会监控pod中部署的容器，在他们失败的时候重新启动他们，但如果整个节点失败，那么节点上的pod会丢失，并不会被新的节点替换。在实际使用中，我们希望pod能自动运行并保持健康，无需干预，所以我们会使用ReplicationController或者deployment一样的资源来创建和管理实际中的pod ”

### 保持pod健康

pod调度到kubernetes的节点中，该节点上的kubelet就会运行pod中的容器，如果pod的主进程崩溃或停止工作，kubelet会重启容器，但是，如果应用程序由于死循环而停止响应（这时候应用程序没办法从内部感知，主程序并没有奔溃），为确保应用程序能在这种情况下重启，就必须从外部检查应用程序的运行状态。

#### 存活探针

kubernetes可以通过存活探针（liveness probe）来检查容器是否还在运行。可以为pod中的每个容器单独指定存活探针。如果探测失败，kubernetes将定期执行探针并重新启动容器。

kubernetes有以下三种探测容器的机制：

- HTTP GET探针，针对容器的IP地址执行HTTP GET请求，如果响应状态码不代表错误，则认为探测成功，如果服务器返回错误响应码或者没有响应，则认为探测失败，容器将被重新启动。
- TCP套接字，尝试与容器指定端口建立TCP连接，如果连接成功则探测成功。
- Exec探针在容器内执行任何命令，并检查命令退出状态码。如果状态码是0，则探测成功。

#### 创建基于HTTP的存活探针

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: kubia-liveness
spec:
  containers:
  - image: luksa/kubia-unhealthy
    name: kubia
    livenessProbe:
      httpGet:
        path: /
        port: 8080
```

`luksa/kubia-unhealthy`是一个被请求五次就会返回500的镜像，我们创建该镜像，在running之后大概2分钟，我们可以通过:

```shell
# kubectl get po kubia-liveness
NAME             READY   STATUS    RESTARTS   AGE
kubia-liveness   1/1     Running   1          2m38s
```

看到他已经重启了。

我们可以通过`kubectl logs kubia-liveness --previous`来看到前一个容器的日志。

也可以通过`kubectl describe po kubia-liveness`来知道为什么容器重启。

ps：可以看到退出状态码为137，137 = 128 + 9（128是无效退出参数，9是终止进程的额信号编号，9是SIGKILL的信号编号），Events中可以看到重启的原因 `Liveness probe failed: HTTP probe failed with statuscode: 500`

#### 配置存活探针的附加属性

通过`kubectl describe po kubia-liveness`我们还可以探针的其他属性

```
Liveness:       http-get http://:8080/ delay=0s timeout=1s period=10s #success=1 #failure=3
```

delay=0s 表示容器启动后立即开始探测，timeout=1s 表示一秒内必须响应，period=10s 表示每过10s探测一次，#failure=3 表示探测失败三次后重启。

定义探针的时候可以自定义这些附加参数。例如，要设置初始延迟，可以将initialDelaySeconds属性添加到存活探针的配置中，如：

```
livenessProbe:
  httpGet:
    path: /
    port: 8080
  initialDelaySeconds: 15
```

如果没有设置初始延迟，则会在启动时开始探测，如果程序还没启动好，并且超过失败阈值，容器就会重启。

### ReplicationController

我们知道kubernetes会在容器崩溃或者存活探针失败的时候，通过重启容器来保持运行，这件事情由承载pod的节点上的kubelet执行，这意味着kubernetes Control Plane组件不会参与此过程。所以当节点本身崩溃，kubelet运行在节点上，kubernetes Control Plane将无法做任何事情。

ReplicationController 是一种kubernetes资源，可以确保它的pod始终保持运行状态。如果pod因任何原因消失，则ReplicationController会注意到缺少了pod并创建替代pod。

ReplicationController会监控pod数量使得它和期待值相同。（通过**标签选择器**来匹配）

```
apiVersion: v1
kind: ReplicationController
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    app: kubia
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: luksa/kubia
        ports:
        - containerPort: 8080
```

注意，pod模板中的labels必须与标签选择器保持一致，不然则意味着永远达不到期待值，pod会无限创建（实际上并不会，API Server做了校验）。更加好的方法是不要去写标签选择器，默认跟pod保持一致。

如果你再创建后修改了标签选择器或者pod模板中的标签，对现有pod不会有任何影响，ReplicationController会创建几个新的。

实际上，直接通过标签选择器来匹配得到Replication Controller的pod是不可靠的，比如你创建了两个标签选择器相同得Replication Controller，他们不应该互相有影响，所以在pod中会有一个metadata.ownerReferences字段来告诉你该pod被谁控制。(**这里是我的观点，跟kubernetes in action中Replication Controller只通过标签选择器来选择pod有点区别，请自行判断。**)

下面代码是kubernetes/dashborad最新的代码（通过namespace取出所有的pod，然后通过`metav1.IsControlledBy(&pod, owner)`去判断pod是不是被ReplicationController控制，实际上我觉得先通过标签过滤，再通过`metav1.IsControlledBy(&pod, owner)`判断会更好，如果这句话有不对的地方，麻烦告诉我）

```go
func getRawReplicationControllerPods(client k8sClient.Interface, rcName, namespace string) ([]v1.Pod, error) {
	rc, err := client.CoreV1().ReplicationControllers(namespace).Get(rcName, metaV1.GetOptions{})
	if err != nil {
		return nil, err
	}

	channels := &common.ResourceChannels{
		PodList: common.GetPodListChannel(client, common.NewSameNamespaceQuery(namespace), 1),
	}

	podList := <-channels.PodList.List
	if err := <-channels.PodList.Error; err != nil {
		return nil, err
	}

	return common.FilterPodsByControllerRef(rc, podList.Items), nil
}
```

```
// FilterPodsByControllerRef returns a subset of pods controlled by given controller resource, excluding deployments.
func FilterPodsByControllerRef(owner metav1.Object, allPods []v1.Pod) []v1.Pod {
	var matchingPods []v1.Pod
	for _, pod := range allPods {
		if metav1.IsControlledBy(&pod, owner) {
			matchingPods = append(matchingPods, pod)
		}
	}
	return matchingPods
}
```

ReplicationController只关心pod是否有某个标签，所以给ReplicationController控制的pod增加标签并不会使得它脱离ReplicationController的管理。

给pod增加标签命令：

```
kubectl label pod kubia-dmdck type=special
```

给pod修改标签命令：

```
kubectl label pod kubia-dmdck app=foo --overwrite
```

列出标签key为app的pod：

```
kubectl get pod -L app
```

缩放ReplicationController的数量为10命令：

```
kubectl scale rc kubia --replicas=10
```

### 使用ReplicaSet而不是ReplcationController

【看到这里，淡定，别打脸！】ReplicaSet只是ReplicationController的高级版而已，实际上我们并不会去直接创建它，我们会创建更高级的deployment（这里为啥要讲？之后在deployment那里会说到）。ReplicationController只能匹配拥有某个标签的pod，ReplicaSet拥有更加牛逼的标签选择器，可以匹配不包含某个标签的，甚至包含某个标签key的pod等等。

#### ReplicaSet标签选择器

```
selector: 
  matchExpressions: 
    - key: app 
      operator: In 
      values. 
        - kubia
```

它拥有四种标签选择器：

1. In : Label的值必须与其中一个指定的values匹配。 
2. Notln : Label的值与任何指定的values不匹配。 
3. Exists : pod必须包含一个指定名称的标签（值不重要）。使用此运算符时， 不应指定values字段。
4. DoesNotExist : pod不得包含有指定名称的标签。values属性不得指定。 

最后插一段kubernetes/dashboard中获取ReplicaSet中pod的代码（这段写得不错）：

```
func getRawReplicaSetPods(client k8sClient.Interface, petSetName, namespace string) ([]v1.Pod, error) {
	rs, err := client.AppsV1().ReplicaSets(namespace).Get(petSetName, metaV1.GetOptions{})
	if err != nil {
		return nil, err
	}

	channels := &common.ResourceChannels{
		PodList: common.GetPodListChannel(client, common.NewSameNamespaceQuery(namespace), 1),
	}

	podList := <-channels.PodList.List
	if err := <-channels.PodList.Error; err != nil {
		return nil, err
	}

	return common.FilterPodsByControllerRef(rs, podList.Items), nil
}
```

```
// FilterPodsByControllerRef returns a subset of pods controlled by given controller resource, excluding deployments.
func FilterPodsByControllerRef(owner metav1.Object, allPods []v1.Pod) []v1.Pod {
	var matchingPods []v1.Pod
	for _, pod := range allPods {
		if metav1.IsControlledBy(&pod, owner) {
			matchingPods = append(matchingPods, pod)
		}
	}
	return matchingPods
}
```

但是它的getReplicaSetPodInfo只通过label selector，应该是有bug的，我提了一个issue： [The Pods status in ReplicaSet is not correct](https://github.com/kubernetes/dashboard/issues/4696)

```
func getReplicaSetPodInfo(client k8sClient.Interface, replicaSet *apps.ReplicaSet) (*common.PodInfo, error) {
	labelSelector := labels.SelectorFromSet(replicaSet.Spec.Selector.MatchLabels)
	channels := &common.ResourceChannels{
		PodList: common.GetPodListChannelWithOptions(client, common.NewSameNamespaceQuery(replicaSet.Namespace),
			metaV1.ListOptions{
				LabelSelector: labelSelector.String(),
				FieldSelector: fields.Everything().String(),
			}, 1),
	}

	pods := <-channels.PodList.List
	if err := <-channels.PodList.Error; err != nil {
		return nil, err
	}

	podInfo := common.GetPodInfo(replicaSet.Status.Replicas, replicaSet.Spec.Replicas, pods.Items)
	return &podInfo, nil
}
```

### 使用Daemonset在每个节点上运行一个pod

